# 第二章 初识 SLAM

## 规划和控制的第一步是感知周边的环境

这里又包括两部分：

​	定位：知道我在什么地方（内部：自身的状态）

​	建图：感知到周围环境是什么样的（外部：外在的环境，地图）



## 传感器

传感器可以分为两类，一类传感器是携带于机器人本体上的，比如机器人的轮式编码器、相机、激光传感器。另一类是安装于环境中的，比如导轨、二维码标志。

这里我们发现安装于环境中的传感器一般会约束外部环境，当约束无法满足时，就没有办法定位。但是在机器人本体上的传感器，它们测量的通常是简洁的物理量而不是直接的位置数据，然后通过这些数据来推算自己的位置，因为这种方法对环境没有要求，更适合于位置环境。



## 视觉SLAM

​	视觉SLAM的相机更加简单，通常不携带昂贵的镜头，而是以一定速率拍摄周围的环境，形成一个连续的视频流

​	按照工作方式的不同，相机可以分为单目（monocular），双目（Stereo）和深度（RGB-D).

### 不同相机用来做SLAM时的特点

#### 单目相机

* 单目相机即只有一个摄像头，单目相机的数据就是一张照片
* 照片的本质是拍摄某个场景在相机的成像平面上留下的一个投影，以二维的形式记录了三维的世界。在这个过程中深度（距离）无法被记录下来。在单目相机中我们无法通过单张图片计算场景中物体于相机之间的距离。
* **距离感/空间感**：帮助我们确定图像中物体的远近关系
* 在单张图片中，无法确定一个物体的真实大小，即可以是一个很大但很远的物体，也可以是一个很近但很小的物体。
* 如果想从二维的平面恢复三维结构，必须改变相机的视角。在单目相机中，必须移动相机，再能估计他的运动和场景中物体的远近和大小（**结构**）。
* **视差（disparity）**：近处的物体移动快，远处的物体移动慢，极远处的物体看上去是不懂得。于是在相机移动的时候，这些物体在图像上的运动就形成了视差。通过视差我们就能定量地判断哪些物体离得远，哪些物体离得近。但这里的远近也是相对的吗，无法确认其真实尺度。
* 单目SLAM估计的轨迹和地图将与真实的轨迹和地图相差一个银子，也就是所谓的**尺度（scale）**。由于单目SLAM无法仅凭图像确定这个真实尺度，所有又称为**尺度不确定性（Scale Ambiguity)**.

#### 双目相机

* 双目相机可以理解为两个单目相机，这两个相机之间的距离被称为**基线（baseline）**，已知。可以通过这个基线来估计每个像素的空间位置。
* 双目相机测量到的深度范围与基线有关，基线距离越大，能够测量到的物体就越远。
* 因为双目相机的距离估计是比较左右眼的图像获得的，并不依赖于其他的传感设备，所以室内室外均可使用。
* 缺点是双目相机的配置与标定都较为复杂，而且视差的计算会非常消耗计算资源。

#### 深度相机

* 通过红外结构光或者Time of Flight(ToF)原理，像激光传感器那样，主动向物体发射光并接受返回的光来测出距离。

* 这里是用物理的测量手段，所以计算资源是大大减少的

* 但是因为它测量范围窄，噪声大，视野小，易受日光干扰，而且无法测量透射材质的距离，主要用在室内

  

## 经典SLAM框架

<img src="C:\Users\Wenyue Wang\AppData\Roaming\Typora\typora-user-images\image-20200703182311701.png" alt="image-20200703182311701" style="zoom: 67%;" />

经典的SLAM流程包括以下步骤：

1. **传感器信息读取**。在视觉SLAM中主要为相机图像信息的读取和预处理。如果是在机器人中，还可能有码盘、惯性传感器等信息的读取和同步
2. **前端视觉里程计(Visual Odometry, VO)**。视觉里程计的任务是估算相邻图像间相机的运动，以及局部地图的样子。VO又被称为前端（front end)
3. **后端（非线性）优化（Optimization）**。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对他们进行优化，得到全局一致的轨迹和地图。由于在VO后面，又被称为后端（back end）
4. **回环检测（Loop Closure Detection）**。回环检测判断机器人是否到达过先前的位置。如果检测到回环，它会把信息提供给后端进行处理。
5. **建图（Mapping）**。它根据估计的轨迹，建立与任务要求对应的地图。

这里如果工作环境是静态、刚体、光照变化不明显、没有人为干扰，这种场景下的SLAM技术已经相当成熟

### 视觉里程计

* 视觉历程计通过相邻帧间的图像估计相机运动，并恢复场景的空间结构
* 这里因为它和实际的里程计一样，只计算相邻时刻的运动，而和过去的信息没有关联
* 尽管用视觉里程计可以将相邻时刻的运动串起来构成机器人的运动轨迹，但是仅通过视觉里程计来估计轨迹会不可避免地出现累积漂移（Accumulating Drift），即累积误差
* 为了解决漂移的问题，我们还需要使用后端优化和回环检测这两种技术，回环检测负责将“机器人回到原始位置”的事情检测出来，后端优化则是根据该信息，校正整个轨迹的形状。

### 后端优化

* 后端优化主要是处理SLAM过程中的噪声问题
* 需要考虑如何从这些带有噪声的数据中估计整个系统的状态，以及这个状态估计的不确定性有多大 -- 最大后验概率估计（Maximum-a-Posteriori, MAP）。这里的状态包括机器人自身的轨迹，也包含地图
* 视觉里程计（前端）给后端提供待优化的数据以及这些数据的初始值，而后端负责整体的优化过程。所以前端和计算机视觉更相关，比如图像的特征提取与匹配，而后端则主要是滤波与非线性优化算法。
* 状态估计问题：对运动主体自身和周围环境空间不确定性的估计

### 回环检测

* 主要是用来解决位置估计随时间漂移的问题
* 假设实际情况下机器人经过一段时间的运动后回到了原点，但是由于漂移，它的位置估计值却没有回到原点。这时如果有某种手段让机器人知道“回到了原点”这件事，或者把“原点”识别出来，我们再把位置估计值“拉”过去，就可以消除飘逸了。
* 回环检测与“定位”和“建图”两者都有密切的联系。为了实现回环检测，我们需要让机器人具有识别到过的场景的能力。我们希望机器人能使用携带的传感器，图像本身，来完成这个任务。比如，通过判断图像间的相似性来完成回环检测。所以说视觉回环检测实质上是一种计算图像数据类似性的算法。
* 在检测到回环之后，我们会把“A,B是同一个点”的信息告诉后端优化算法，然后后端就可以根据这些信息，将轨迹和地图调整到符合回环检测结果的样子，从而消除累计误差。

### 建图

* 建图指构建地图的过程，地图是对环境的描述

* 地图可以分为度量地图和拓扑地图两种

  * ##### 度量地图（Metric Map）

    * 度量地图强调精确地表示地图中物体的位置关系，一般用稀疏（sparse）和稠密（dense）来分类

    * ##### 稀疏

      * 稀疏地图是由路标（landmark）组成的地图，不是路标的部分就可以忽略掉了
      * 定位时可以用稀疏地图

    * ##### 稠密

      * 稠密地图侧重于建模所有看到的东西
      * 导航时需要用稠密地图
      * 稠密地图通常按照某种分辨率，由许多个小块组成。在二维度量地图中体现为多个小格子（grid），而在三维度量地图中则体现为许多小方块（Voxel）。
      * 通常一个小块由三种状态：占据、空闲、未知，来表达这个块中是否有物体。这样当查询某个空间位置时，地图能够给出该位置是否可以通过的信息。
      * 缺点：需要储存每一个格点的状态，会耗费大量的存储空间，所述情况下地图的一些细节部分是无用的。另一方面大鬼为度量地图有时会出现一致性问题，很小的一点转向误差就可能会导致两间屋子的墙出现重叠，使地图失效。

  * ##### 拓扑地图（Topological Map)

    * 拓扑地图不关注精确性，而更强调地图元素之间的关系。
    * 拓扑地图是一个图（Graph），由节点和边组成。这时我们只关注两点是否联通，而不考虑如果从一点到另一点。



## SLAM问题的数学表述

### 参数设置

假设机器人携带着某种传感器在位置环境里运动，怎么用数学语言描述？

* 运动时间：将一段连续时间的运动转变为离散时刻（$t=1. 2. ..., K$）中发生的事情
* 这些时刻机器人的自身位置：$x_1, x_2, ..., x_K$
* 地图上一共有N个路标点，每个时刻测到的路标点数量：$y_1, y_2, ..., y_N$

### 运动描述

描述“机器人携带着传感器在环境中运动”：

* 运动：从 $K-1$ 时刻到 $K$ 时刻，机器人的位置 $x$ 是如何变化的
* 观测：假设机器人在 $K$ 时刻于 $x_K$ 位置探测到了某一个路标 $y_j$， 那么如何用数学语言来描述？

#### 运动方程

$$
x_k=f(x_{k-1}, u_k,w_k)
$$

* $u_k$：运动传感器的读数，两个时间点运动的变化量
* $w_k$：运动过程中加进去的噪声
* $x_k$：机器人的当前位置
* $x_{k-1}$：机器人上一个时刻的位置

这里我们用 $f$ 表示一个通用的方程，使得这个函数可以指代任意的运动传感器/输入

这里噪声的存在让这个模型变成了随机模型，因为没有噪声的话所有的指令都是准确的

#### 观测方程

描述机器人在 $x_k$ 位置上看到某个路标点 $y_j$ 时，产生了一个观测数据 $z_{k,j}$
$$
z_{k,j}=h(y_j,x_k,v_{k,j})
$$
这里 $v_{k,j}$ 使这次观测里的噪声。因为观测所用的传感器形式有很多，这里的观测数据 $z$ 以及观测方程 $h$ 也有很多种不同的形式

#### 参数化（Parameterization）

这里以线性运动为例：

假设机器人在平面中运动，那么它的位姿由两个位置和一个转角来描述，即
$$
x_k=[x_1,x_2,\theta]_k^T
$$
这里 $x_1, x_2$ 是两个轴上的位置而 $\theta$ 为转角。同时两个时间点运动的变化量为：
$$
u_k=[\Delta x_1, \Delta x_2, \Delta \theta]_k^T
$$
那么这里机器人的线性运动的运动方程就可以表示为：
$$
\begin{bmatrix}
    x_1 \\
    x_2 \\
    \theta
  \end{bmatrix}_k =
  \begin{bmatrix}
    x_1 \\
    x_2 \\
    \theta
  \end{bmatrix}_{k-1} + 
  \begin{bmatrix}
    \Delta x_1 \\
    \Delta x_2 \\
    \Delta \theta
  \end{bmatrix}_k + w_k
$$
对于观测方程，假设机器人携带着一个二维激光传感器。当这个传感器观测到一个2D路标点时，能够测量到路标点和机器人之间的距离 $\tau$ 和两者的夹角 $\phi$。这里：
$$
路标点：y_j=[y_1,y_2]_j^T \\
位姿：x_k=[x_1,x_2]_k^T \\
观测数据：z_{k,j}=[\tau _{k,j},\phi _{k,j}]^T
$$
那么我们可以将观测方程写为：
$$
\begin{bmatrix}
    \tau _{k,j} \\
    \phi _{k,j} 
  \end{bmatrix} =
  \begin{bmatrix}
    \sqrt{(y_{1,j}-x_{1,k})^2+(y_{2,j}-x_{2,k})^2} \\
    arctan(\frac{y_{2,j}-x_{2,k}}{y_{1,j}-x_{1,k}} ) 
  \end{bmatrix}+v
$$
如果将运动方程和观测方程写在一起，那么我们可以得到：
$$
\begin{cases}
x_k=f(x_{k-1}, u_k,w_k)& k=1,2,...,K\\
z_{k,j}=h(y_j,x_k,v_{k,j})& (k,j)\in O
\end{cases}
$$
这里O是一个集合，记录着在那个时刻观察到了哪个路标。

这两个方程描述了最基本的SLAM问题：当我们知道运动测量的读数 u 以及传感器的读数 z 时，如何求解定位问题（估计 x 的值）和建图问题（估计 y）。

#### 状态估计问题

如何通过带有噪声的测量数据，估计内部的、隐藏着的状态变量？

* 线性/非线性：运动和观测方程是否为线性
* 高斯/非高斯：噪声是否服从高斯分布

那么对于：

* 线性高斯系统（Linear Gaussian, LG系统）：卡尔曼滤波器求解（Kalman Filter, KF)
* 非线性非高斯系统（Non-Linear, Non-Gaussian, NLNG）：扩展卡尔曼滤波器（Extended Kalman Filter, EKF）和非线性优化这两种方法来求解
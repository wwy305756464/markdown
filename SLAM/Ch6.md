# 非线性优化

​	SLAM有运动方程和观测方程：
$$
x_k=f(x_{k-1},u_k)+w_k \\
z_{k,j}=h(y_j,x_k)+v_{k,j}
$$
​	这里 $x$ 代表了相机的位姿，可以用变换矩阵（SE(3)）描述，然后用李代数进行细节方面调整的优化。观测方程由针孔相机的模型给定，代表了在 $x_k$ 处对路标 $j_y$ 进行了依次观测，这时对应到图上的像素位置 $z_{k,j}$. 这里的内参由相机决定，外参是相机的位姿。但是上述描述都是在 noise free 的情况下讨论的，在加上 $w, v$ 这两种噪声后，我们需要讨论如何在有噪声的数据中进行准确的状态估计。

## 状态估计问题

### 批量状态估计与最大后验估计

#### 运动方程和观测方程

​	我们知道运动方程和观测方程：
$$
x_k=f(x_{k-1},u_k)+w_k \\
z_{k,j}=h(y_j,x_k)+v_{k,j}
$$

##### 参数

​		$x_k, \space x_{k-1}$：机器人的实际位置，可以用变换矩阵（SE(3)）描述，用李代数进行微调

​		$u_k$：运动传感器的读数，读出来的两个时间点上运动的变化量

​		$w_k$：运动过程中加进去的噪声

​		$z_{k,j}$：在 k 时刻，y 路标点在相机上成像的坐标

​		$y_j$：路标点 j

​		$z_{k,j}$：观测过程中的噪声

##### 相机模型

如果将观测方程写成相机模型中的形式 ，则为：
$$
zP_{uv}=K(RP_w+t)\space \space \to sz_{k,j}=K(R_ky_j+t_k)
$$
​	这里 $s$ 为像素点的距离，$z_{k,j}$ 为路标在图上的像素位置，$K$ 为相机的内参矩阵，$y_j$ 为路标点在世界坐标系下的坐标，$R_k, t_k$ 为外参（旋转，平移）。

##### 加入噪声项

​	现在我们将两个噪声项（$w_k,v_{k,j}$）加入考虑，并且假设两者符合零均值的高斯分布：
$$
w_k \sim \mathcal{N}(0,R_k), \space \space v_k\sim\mathcal{N}(0,Q_{k,j})
$$
​	这里 $\mathcal{N}$ 代表高斯分布，0 代表零均值，$R_k,Q_{k,j}$ 是协方差矩阵。

​	在这些噪声得影响下，我们希望通过带噪声的数据 $z$ 和 $u$ 来推断出机器人的位姿 $x$ 和地图 $y$（以及他们的概率分布）。这就是一个状态估计问题。

#### 状态估计问题的处理方法

##### 增量/渐进（incremental）/ 滤波器：

* 持有一个当前时刻的估计状态，然后用新的数据来更新
* 卡尔曼滤波器
* 仅关心当前时刻的状态估计 $x_k$，之前的状态不予考虑

##### 批量（batch）：

* 将数据攒起来一并处理
* 可以在更大的范围达到最优化，比如让机器人收集所有时刻的数据之后统一处理，但是这样不是实时处理

##### 滑动窗口估计法：

* 固定一些历史轨迹，仅对当前时刻附近的一些轨迹进行优化

  

#### 非线性的批量优化

​	假设在 1-N 的时刻中有 M 个路标点，那么机器人位姿和路标点可以表示为：
$$
x=\{x_1,x_2,...,x_N\} \space , \space y=\{y_1,y_2,...,y_M\}
$$
​	那么这里一直的数据为传感器输出的所有时刻的输出数据 $u$ 和所有时刻相机观测到的路标点的数据 $y$。希望求的量为机器人的实际位置 $x$ 和路标的实际位置 $y$。所以对机器人状态的估计可以简化为在一直输入数据 $u$ 和观测数据 $z$ 的条件下，求状态 $x,y$ 的条件概率分布：
$$
P(x,y|z,U)
$$
​	如果我们不知道传感器的输入数据，则估计$P(x,y|z)$，被称为sfM，从许多图像中重建三维空间结构。

##### 计算P：

$$
\underbrace{P(x,y|z,u)}_{后验概率}=\frac{P(z,u|x,y)P(x,y)}{P(z,u)}\propto \underbrace{P(z,u|x,y)}_{似然\\Likehood}\underbrace{P(x,y)}_{先验\\Prior}
$$

​	这里直接求后验分布是困难的，但是求一个状态最优估计，使得在该状态下后验概率最大化是可行的：
$$
(x,y)^*_{\text{MAP}}=\arg\max P(x,y|z,u)=\arg\max P(u,z|x,y)P(x,y)
$$
​	即求解最大后验概率等价于最大化似然和先验的乘积。

##### 求解最大似然估计（Maximize Likelihood Estimation，MLE）

​	如果我们不知道机器人和路标大概在什么地方，即 $P(x,y)$ 未知，那么我们不用考虑先验，可以求解MLE：
$$
(x,y)^*_{\text{MLE}}=\arg\max P(z,u|x,y)
$$
​	似然（Likelihood）指在现在的位姿下，可能产生怎样的观测数据。在这里我们已知的是观测数据，那么最大似然估计可以理解为：“在什么样的状态下，最可能产生现在观测到的数据。“



### 最小二乘

#### 观测模型和观测数据的条件概率：

对于某一次观测：
$$
z_{k,j}=h(y_j,x_k)+v_{k,j}
$$
加入噪声项之后，观测数据的条件概率为：
$$
P(z_{j,k}|x_k,y_j)=\mathcal N(h(y_j,x_k),Q_{k,j}
$$
这里我们用最小化负对数来求一个高斯分布的最大似然。

代入SLAM的观测模型，并且经过相关推导之后我们可以得到：
$$
(x_k,y_j)^*=\arg\max \mathcal N(h(y_j,x_k),Q_{k,j})\\=\arg\min ((z_{k,j}-h(x_k,y_j))^TQ_{k,j}^{-1}(z_{k,j}-h(x_k,y)))
$$
上面公式是针对某一个时刻的观测模型，现在我们考虑批量时刻的数据，假设各个时刻的输入和观测是相对独立的，那么各个输入见是独立的，各个观测间也是独立的。所以我们可以得到：
$$
P(z,u|x,y)=\prod_kP(u_k|x_{k-1},x_k)\prod_{k,j}P(z_{k,j}|x_k,y_j)
$$
这说明了我们可以独立处理各时刻的运动和观测。

##### 误差

我们定义各次输入和观测数据与模型之间的误差：
$$
e_{u,k}=x_k-f(x_{k-1},u_k) \\
e_{z,j,k}=z_{k,j}-h(x_k,y_j)
$$
将各个时刻的误差累积起来就得到了一个最小二乘问题：
$$
\min J(x,y)=\sum_k e_{u,k}^TR_k^{-1}e_{u,k}+\sum_k\sum_ke^{-1}_{z,k,j}Q^{-1}_{k,j}e_{z,k,j}
$$
我们需要解决的就是如何求解这个最小二乘问题



### 例子：批量状态估计

考虑有一辆汽车在沿 x 轴方向前进或后退地运动，那么运动和观测方程为：
$$
运动：\space x_k=x_{k-1}+u_k+w_k, \space\space\space\space w_k\sim\mathcal{N}(0,Q_k)\\
观测：\space z_k=x_k+n_k,\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space\space n_k\sim\mathcal{N}(0,R_k)
$$
这里每一项代表：

* $x_k$：k 时刻汽车的位置
* $x_{k-1}$：k-1 时刻汽车的位置
* $u_k$：k 时刻传感器读出的与上个时刻之间的差值
* $w_k$：运动中的噪声（误差）
* $z_k$：汽车位置的测量值
* $n_k$：观测中的噪声

这里假设汽车经过了三个时刻，k=1,2,3，而且初始状态 $x_0$ 已知：
$$
x=\begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3\end{bmatrix}, \quad z=\begin{bmatrix}z_1\\z_2\\z_3\end{bmatrix},\quad u=\begin{bmatrix}u_1\\u_2\\u_3\end{bmatrix}
$$
根据之前的推导我们知道最大似然估计为：
$$
x^*_{\text map}=\arg\max P(x|u,z)=\arg\max P(u,z|x)=\prod_{k=1}^3P(u_k|x_{k-1},x_k)\prod_{k=1}^3P(z_k|x_k)
$$
我们从运动方程和观测方程已知：
$$
P(z_k|x_k)=\mathcal N(x_k, R_k) \\
P(u_k|x_{k-1},x_k)=\mathcal N(x_k-x_{k-1},Q_k)
$$
同样，我们构建误差变量：
$$
e_{u,k}=x_k-x_{k-1}-u_k \\
e_{z,k}=z_k-x_k
$$
整理得知，最小二乘的目标函数为：
$$
\min \sum_{k=1}^3 e_{u,k}^T Q_k^{-1} + \sum_{k=1}^3 e_{z,k}^T R_{k}^{-1} e_{z,k}
$$

因为这里是线性系统，我们可以将其写成向量形式：$y=\begin{bmatrix}u\\z\end{bmatrix}$。所以可以写成：
$$
y-Hx=e\sim\mathcal N(0,\Sigma)
$$
其中：
$$
H=\begin{bmatrix} 1 && -1 && 0 && 0 \\ 0 && 1 && -1 && 0 \\0 && 0 && 1 && -1 \\
0 && 1 && 0 && 0 \\ 0 && 0 && 1 && 0 \\ 0 && 0 && 0 && 1\end{bmatrix} \\
\Sigma=\operatorname{diag}(Q_1,Q_2,Q_3,R_1,R_2,R_3)
$$

所以可以将这个问题写成：
$$
x^*_{map}=\arg\min e^T\Sigma^{-1}e
$$
那么这个问题有唯一的解：
$$
x_{map}^*=(H^T\Sigma^{-1}H)^{-1}H^T\Sigma^{-1}y
$$


## 最小二乘问题（Least Square）

<img src="https://pic2.zhimg.com/80/v2-68fd43d677198036136ae721234d12eb_1440w.jpg?source=1940ef5c" alt="img" style="zoom: 67%;" />

​	这里 $y$ 是真实值，$y_1,y_2,y_3,y_4,y_5$ 为测量值。真实值是未知的，我们想通过这些测量值来得到最精确的真实值。那么每个测量值的误差就可以看作：$(y-y_i)^2$，这里用平方可以省略绝对值，总的误差就是$\sum(y-y_i)^2$。那么我们的问题就变为求最小的总误差，这里对总误差求导并使导数为0即可。

​	上面的例子只有一个系数，如果有多个系数，可以最总误差求partial derivative，求出理想的每个系数



## 非线性最小二乘

​	线性的最小二乘问题，我们可以轻易通过求导取0来求解，那么对于形式复杂的导函数，求解这个方程我们需要知道关于目标函数的全局性质，通常这是不太可能的。因此我们可以用迭代的方法，从一个初始值出发，不断地更新当前的优化变量，使目标函数下降

##### 步骤：

1. 给定某个初始值 $x_0$
2. 对于第 k 次迭代，寻找一个增量 $\Delta x_k$，使得 $||f(x_k+\Delta x_k)||^2_2$ 达到极小值
3. 如果 $\Delta x_k$ 足够小，则停止
4. 否则，令 $x_{k+1}=x_k+\Delta x_k$，返回第二步

这里我们其实将求解导函数为0的问题变成了一个不断寻找下降增量 $\Delta x_k$ 的问题。这里我们发现如果对 f 进行线性优化，增量的计算会简单很多。而且当函数下降到增量非常小的时候，我们就认为函数开始收敛，咪表函数也达到了一个极小值，所以我们只需要关心 f 在迭代值处的局部性质而非全局性质。所以我们的重点会放在如何寻找这个增量 $\Delta x_k$。

### 一阶和二阶梯度法

假设在第 $k$ 次迭代，我们在 $x_k$ 处，想要寻找增量 $\Delta x_k$。我们将目标函数在 $x_k$ 附近做泰勒展开：
$$
F(x_k+\Delta x_k)\approx F(x_k)+J(x_k)^T\Delta x_k + \frac{1}{2}\Delta x_k^T H(x_k)\Delta x_k
$$
这里 $J$ 是 $F(x)$ 关于 $x$ 的一阶导数（Jacobian），$H$ 是 $F(x)$ 关于 $x$ 的二阶导数（Hessian）。

#### 一阶梯度法

​	第一种方法是保留泰勒展开的一阶项，即一阶梯度法，取增量为反向的梯度，即可保证函数下降，增量为：
$$
\Delta x^*=-J(x_k)
$$

#### 最速下降法

​	在一阶梯度法的基础上指定一个步长 $\lambda$，步长根据一定条件来计算，即为最速下降法。只要我们沿着反向梯度方向前进，在一阶（线性）的近似下，目标函数肯定会下降。

#### 二阶梯度法

​	如果保留泰勒展开的二阶梯度信息，则这里的增量为：
$$
\Delta x^*=\arg\min (F(x)+J(x)^T\Delta x + \frac{1}{2}\Delta x^T H \Delta x)
$$
​	求解这个函数即求解右侧等式关于 $\Delta x$ 的导数并令它为零，得到：、
$$
J+H\Delta x=0 \to H\Delta x=-J
$$
​	求解这个线性方程我们就得到了增量，这个也叫做牛顿法。



### 高斯牛顿法

